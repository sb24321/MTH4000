{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        # instantiates the lemmatizer, which is important for reducing word to base form\n",
    "        self.__lemmatizer = WordNetLemmatizer()\n",
    "        self.__vocab = []  # holds all the words that are understood by the model\n",
    "        self.__classes = []  # holds all the classes from the intents file\n",
    "        # matches each possible input with the associated class.\n",
    "        self.__documents = []\n",
    "        # a list of characters to ignore. (adds noise without adding value)\n",
    "        self.__IGNORE_CHAR = [\"?\", \"!\", \".\", \",\"]\n",
    "        # data that is used for training, a vectorised form of all the recognised inputs\n",
    "        self.__training = []\n",
    "        self.__model = None  # holds the actual trained model.\n",
    "\n",
    "    def create_model(self, file_to_load_from: str, file_to_save_to=\"model.h5\"):\n",
    "        \"\"\"\n",
    "        Creates a model from a file and saves that model to another file\n",
    "\n",
    "        Args:\n",
    "            file_to_load_from (str): The name of the file to which the original data is loaded from. This is a .json file\n",
    "            {\"intents\": [{\"tag\": <tag-name>, \"patterns\": [<responses-to-look-for>], \"responses\": [<reponses-of-the-machine>]}], ...}\n",
    "\n",
    "            file_to_save_to (str, optional): The file to which the model will be saved to. Defaults to 'model.h5'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__file_to_load_from = file_to_load_from\n",
    "\n",
    "        self.__load_intents()  # loads the intents from the file\n",
    "        self.__preprocess_data()  # preprocess and cleans the data\n",
    "        self.__create_training_data()  # creates the training data\n",
    "        self.__create_model()  # creates the model\n",
    "        self.__train_model()  # trains the model\n",
    "        self.__save_model(model_file=file_to_save_to)  # saves the model\n",
    "\n",
    "    def __load_intents(self):\n",
    "        \"\"\"\n",
    "        Loads the model from the .json file\n",
    "        \"\"\"\n",
    "\n",
    "        # loads the model from the file\n",
    "        with open(self.__file_to_load_from, \"r\") as file:\n",
    "            self.__intents = json.load(file)\n",
    "\n",
    "    def __preprocess_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocesses the data by tokenizing the patterns, cleanse the data, adding words to the machine's vocabulary, and creating a list of documents with their corresponding tags.\n",
    "        \"\"\"\n",
    "\n",
    "        # iterates through each intent passed from the intents file.\n",
    "        for intent in self.__intents[\"intents\"]:\n",
    "            # FOR EACH PATTERN:\n",
    "            #   tokenise the pattern to make a list of words\n",
    "            #   add those words to the machines vocabulary.\n",
    "            #   append the list and tag to that word list, ([<tokenised input>], <class-name>)\n",
    "\n",
    "            for pattern in intent[\"patterns\"]:\n",
    "                wordList = pattern.split()\n",
    "                # adds all the words from the pattern to the vocab\n",
    "                self.__vocab.extend(wordList)\n",
    "                self.__documents.append((wordList, intent[\"tag\"]))\n",
    "\n",
    "                # if the tag is not in the classes list, then add the tag\n",
    "                if intent[\"tag\"] not in self.__classes:\n",
    "                    self.__classes.append(intent[\"tag\"])\n",
    "\n",
    "        # for each word in the vocabulary, use the lemmatizer to remove any unneccesary suffixes\n",
    "        self.__vocab = [\n",
    "            self.__lemmatizer.lemmatize(word)\n",
    "            for word in self.__vocab\n",
    "            if word not in self.__IGNORE_CHAR\n",
    "        ]\n",
    "\n",
    "        # create a sorted set of the vocab, and a sorted list of the classes\n",
    "        self.__vocab = sorted(set(self.__vocab))\n",
    "        self.__classes = sorted(self.__classes)\n",
    "\n",
    "    def __create_training_data(self):\n",
    "        \"\"\"\n",
    "        Vectorises the data and splits the data into 'bags' and 'rows'.\n",
    "        \"\"\"\n",
    "        # iterates through each item in the self.__documents\n",
    "        for document in self.__documents:\n",
    "            # Matrix where each row represents the vocabulary.\n",
    "            # If the word in the vocabulary is also in the document, then that will be 1 to show that it is present in both.\n",
    "            bag = []\n",
    "\n",
    "            # gets the tokenised words\n",
    "            word_patterns = document[0]\n",
    "\n",
    "            # converts all words to lowercase and the lemmatizer reduces it to its base dictionary form\n",
    "            word_patterns = [\n",
    "                self.__lemmatizer.lemmatize(word.lower()) for word in word_patterns\n",
    "            ]\n",
    "\n",
    "            # iterates through each word in vocab\n",
    "            # create a vectorised from of the user input\n",
    "            # against the vocabulary, if there is an instance in the user input, make that associated index 1.\n",
    "            for word in self.__vocab:\n",
    "                bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "            # creates an output row that is as long as the length of the classes\n",
    "            # each index corresponds to a class\n",
    "            # sets the index corresponding to the class tag to 1\n",
    "            output_row = list([0] * len(self.__classes))\n",
    "            output_row[self.__classes.index(document[1])] = 1\n",
    "\n",
    "            # append the bag of word and the output row to the training list as one list\n",
    "            self.__training.append(bag + output_row)\n",
    "\n",
    "        # shuffles the data, forces the model to learn patterns from relationships rather than sequence of data\n",
    "        random.shuffle(self.__training)\n",
    "\n",
    "        # converts into a numpy array, more efficent and can be easily manipulated, which is needed from training\n",
    "        self.__training = np.array(self.__training)\n",
    "\n",
    "        # splits the data into the bag (train_x) and row (train_y)\n",
    "        self.__train_x = self.__training[:, : len(self.__vocab)]\n",
    "        self.__train_y = self.__training[:, len(self.__vocab) :]\n",
    "\n",
    "    def __create_model(self):\n",
    "        \"\"\"\n",
    "        Prepares the model for training by creating, optimising and then compiling the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepares the model for training #\n",
    "\n",
    "        self.__model = tf.keras.Sequential(\n",
    "            [\n",
    "                # creates the first later with 128 neurons\n",
    "                # input_shape tells the model of how many inputs to expect.\n",
    "                # uses a Reftified Linear Unit, which allows the model to learn from more complex patterns\n",
    "                tf.keras.layers.Dense(\n",
    "                    128, input_shape=(len(self.__train_x[0]),), activation=\"relu\"\n",
    "                ),\n",
    "                # Sets half the inputs to 0, so that it learns patterns rather than details\n",
    "                # makes predictions less reliant on a single neuron/node\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                # Creates the second later with 64 neurons\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                # preventing overfitting again\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                # output layer, with the same number of neutrons as the output vector\n",
    "                # softmax used to give a probability over the differnt classes, that sum to 1.\n",
    "                tf.keras.layers.Dense(len(self.__train_y[0]), activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # optimises the model\n",
    "        # learning  => controls the size of the steps the optimizer uses when balancing the weights\n",
    "        # momentum  => dictates how much of the previous data is used to make the current update\n",
    "        # nesterov  => updates the weights and makes corrections quickly, results in faster processing.\n",
    "\n",
    "        sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "        # Prepares the model for training.\n",
    "        # loss      => how good the model is compared to the actual data, measures the difference between the predicted probabilities and the actual distribution\n",
    "        # optimizer => changes the attribues liek weight and learning rate reduce loss\n",
    "        # metrics   => used to evaluate performace, how correct its predictions are\n",
    "\n",
    "        self.__model.compile(\n",
    "            loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    def __train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Responsible for training the model using the data.\n",
    "        # epoch      => how many times the model will iterate over the training databset\n",
    "        # batch_size => model will update its weights after 5 samples\n",
    "        # verbose    => showing the information from training (0 -> show nothing, 1-> show progress bar, 2 -> more in depth)\n",
    "        self.hist = self.__model.fit(\n",
    "            self.__train_x, self.__train_y, epochs=200, batch_size=5, verbose=1\n",
    "        )\n",
    "\n",
    "    def __save_model(self, model_file=\"model.h5\"):\n",
    "        \"\"\"\n",
    "        Saves the model, words and classes to a file, which is needed for generating responses.\n",
    "\n",
    "        Args:\n",
    "            model_file (str, optional): The name of the file that the model will be saved to. Defaults to 'model.h5'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Saves the models, words and classes\n",
    "        self.__model.save(model_file, self.hist)\n",
    "        with open(\"vocab.pkl\", \"wb\") as file:\n",
    "            pickle.dump(self.__vocab, file)\n",
    "        with open(\"classes.pkl\", \"wb\") as file:\n",
    "            pickle.dump(self.__classes, file)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ResponseGeneration:\n",
    "    def __init__(self, intents='intents.json', model_file='model.h5'):\n",
    "        self.__CONNECTIVES = [\n",
    "                                \"also\",\n",
    "                                \"furthermore\",\n",
    "                                \"moreover\",\n",
    "                                \"additionally\",\n",
    "                                \"in addition\",\n",
    "                                \"likewise\",\n",
    "                                \"as well as\",\n",
    "                                \"together with\",\n",
    "                                \"alongside\",\n",
    "                                \"besides\",\n",
    "                                \"plus\",\n",
    "                                \"too\",\n",
    "                                \"in the same way\",\n",
    "                                \"likewise\",\n",
    "                                \"similarly\",\n",
    "                                \"coupled with\",\n",
    "                                \"furthermore\",\n",
    "                                \"equally\",\n",
    "                                \"and\"\n",
    "                            ]\n",
    "\n",
    "\n",
    "        self.__lemmatizer = WordNetLemmatizer() # instantiates the lemmatizer, which is important for reducing word to base form\n",
    "\n",
    "        # reads the intents, vocabulary and classes file\n",
    "        self.__intents = json.loads(open(intents).read()) \n",
    "        self.__vocab = pickle.load(open('vocab.pkl', 'rb'))\n",
    "        self.__classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "\n",
    "        # also loads the model from the file\n",
    "        self.__model = tf.keras.models.load_model(model_file)\n",
    "\n",
    "    def __clean(self, sentence: str) -> list:\n",
    "        \"\"\"\n",
    "        Cleanses and tokenises the data.\n",
    "\n",
    "        Args:\n",
    "            sentence (str): The sentence to cleanse.\n",
    "\n",
    "        Returns:\n",
    "            list: The cleansed, tokenised data.\n",
    "        \"\"\"\n",
    "\n",
    "        # tokenises and lemmatizes (means to revert all words to base form) the user input\n",
    "        sentence_words = sentence.split()\n",
    "\n",
    "        # lemmatizes each word in the text\n",
    "        sentence_words = [ self.__lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "\n",
    "        # return the tokenised, lemmatized text\n",
    "        return sentence_words\n",
    "\n",
    "    def __bag_of_words(self, sentence):\n",
    "\n",
    "        # Cleans and tokenises the data\n",
    "        sentence_words = self.__clean(sentence)\n",
    "\n",
    "        # Create a bag as long as the vocabulary\n",
    "        bag = [0] * len(self.__vocab)\n",
    "\n",
    "        # create a vectorised from of the user input\n",
    "        # against the vocabulary, if there is an instance in the user input, make that associated index 1.\n",
    "        for w in sentence_words:\n",
    "            for i, word in enumerate(self.__vocab):\n",
    "                if word == w:\n",
    "                    bag[i] = 1\n",
    "\n",
    "        # return the vectorised form of the user input\n",
    "        return np.array(bag)\n",
    "\n",
    "    def __predict_class(self, sentence):\n",
    "        \n",
    "        # turns the input into a vector\n",
    "        bow = self.__bag_of_words(sentence) \n",
    "\n",
    "        # uses the pre-trained model to predict the class of the input sentence \n",
    "        # [class1, class2, class3]\n",
    "        # [ prob1,  prob2,  prob3], sum = 1 \n",
    "        res = self.__model.predict(np.array([bow]))[0]\n",
    "\n",
    "        # only uses responses that have a probability >0.25 are chosen\n",
    "        error_thresh = 0.25\n",
    "        results = [[i, r] for i, r in enumerate(res) if r > error_thresh]\n",
    "\n",
    "        # sorts so the greatest probability is in index 0\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # for result in results, create a dictionary of the intent and the probability of the intent.\n",
    "        # stores the dictioanry in a list\n",
    "        return_list = [{'intent': self.__classes[r[0]], 'probability': str(r[1])} for r in results]\n",
    "        \n",
    "        return return_list\n",
    "\n",
    "    def __find_response(self, intents_list):\n",
    "\n",
    "        # gets the tag of the most probable response\n",
    "        tag = intents_list[0]['intent']\n",
    "\n",
    "        # go through all the data\n",
    "        for intent in self.__intents['intents']:\n",
    "            # find the tag for a piece of data is the tag of the most probable response\n",
    "            # return a random response from the list of responses it has\n",
    "            if intent['tag'] == tag:\n",
    "                return random.choice(intent['responses'])\n",
    "            \n",
    "        # cannot find\n",
    "        return \"I am sorry, I do not understand the question. Could you try and rephrase it for me please.\"\n",
    "\n",
    "    def get_response(self, message):\n",
    "        \n",
    "        # able to handle multiple requests.\n",
    "        # <request> ::= <request><connective><request>\n",
    "\n",
    "        input_components, component = [], []\n",
    "        \n",
    "        # splits the message into in tokens and iterates through.\n",
    "        # splits the input by connectives to form individiual requests.\n",
    "        for word in message.split():\n",
    "\n",
    "            # if it is not a connective, then add that word to the component list\n",
    "            if word not in self.__CONNECTIVES: component.append(word)\n",
    "\n",
    "            # if that word is a connective, then that is one complete request\n",
    "            # add that to the input component list and reset the componnet list\n",
    "            else:\n",
    "                input_components.append(\" \".join(component))\n",
    "                component = []\n",
    "\n",
    "        # add the trailing request to the component list\n",
    "        input_components.append(\" \".join(component))\n",
    "\n",
    "        # initialises a list for all full response.\n",
    "        full_response = []\n",
    "\n",
    "        # iterates through each input component and generate a response for that componenet.\n",
    "        # adds the return componented to the response list.\n",
    "        input(input_components)\n",
    "        for r in input_components:\n",
    "\n",
    "            # predicts the class of the given class, thus establish the context of the input.\n",
    "            intents_list = self.__predict_class(r)\n",
    "            full_response.append(self.__find_response(intents_list))\n",
    "\n",
    "        # returns the response.\n",
    "        return \"\\n\".join(full_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rg = ResponseGeneration()\n",
    "\n",
    "    rg.get_response(\"Hello, who are)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
