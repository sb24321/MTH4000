{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        # instantiates the lemmatizer, which is important for reducing word to base form\n",
    "        self.__lemmatizer = WordNetLemmatizer()\n",
    "        self.__vocab = []  # holds all the words that are understood by the model\n",
    "        self.__classes = []  # holds all the classes from the intents file\n",
    "        # matches each possible input with the associated class.\n",
    "        self.__documents = []\n",
    "        # a list of characters to ignore. (adds noise without adding value)\n",
    "        self.__IGNORE_CHAR = [\"?\", \"!\", \".\", \",\"]\n",
    "        # data that is used for training, a vectorised form of all the recognised inputs\n",
    "        self.__training = []\n",
    "        self.__model = None  # holds the actual trained model.\n",
    "\n",
    "    def create_model(self, file_to_load_from: str, file_to_save_to=\"model.h5\"):\n",
    "        \"\"\"\n",
    "        Creates a model from a file and saves that model to another file\n",
    "\n",
    "        Args:\n",
    "            file_to_load_from (str): The name of the file to which the original data is loaded from. This is a .json file\n",
    "            {\"intents\": [{\"tag\": <tag-name>, \"patterns\": [<responses-to-look-for>], \"responses\": [<reponses-of-the-machine>]}], ...}\n",
    "\n",
    "            file_to_save_to (str, optional): The file to which the model will be saved to. Defaults to 'model.h5'.\n",
    "        \"\"\"\n",
    "\n",
    "        self.__file_to_load_from = file_to_load_from\n",
    "\n",
    "        self.__load_intents()  # loads the intents from the file\n",
    "        self.__preprocess_data()  # preprocess and cleans the data\n",
    "        self.__create_training_data()  # creates the training data\n",
    "        self.__create_model()  # creates the model\n",
    "        self.__train_model()  # trains the model\n",
    "        self.__save_model(model_file=file_to_save_to)  # saves the model\n",
    "\n",
    "    def __load_intents(self):\n",
    "        \"\"\"\n",
    "        Loads the model from the .json file\n",
    "        \"\"\"\n",
    "\n",
    "        # loads the model from the file\n",
    "        with open(self.__file_to_load_from, \"r\") as file:\n",
    "            self.__intents = json.load(file)\n",
    "\n",
    "    def __preprocess_data(self) -> None:\n",
    "        \"\"\"\n",
    "        Preprocesses the data by tokenizing the patterns, cleanse the data, adding words to the machine's vocabulary, and creating a list of documents with their corresponding tags.\n",
    "        \"\"\"\n",
    "\n",
    "        # iterates through each intent passed from the intents file.\n",
    "        for intent in self.__intents[\"intents\"]:\n",
    "            # FOR EACH PATTERN:\n",
    "            #   tokenise the pattern to make a list of words\n",
    "            #   add those words to the machines vocabulary.\n",
    "            #   append the list and tag to that word list, ([<tokenised input>], <class-name>)\n",
    "\n",
    "            for pattern in intent[\"patterns\"]:\n",
    "                wordList = pattern.split()\n",
    "                # adds all the words from the pattern to the vocab\n",
    "                self.__vocab.extend(wordList)\n",
    "                self.__documents.append((wordList, intent[\"tag\"]))\n",
    "\n",
    "                # if the tag is not in the classes list, then add the tag\n",
    "                if intent[\"tag\"] not in self.__classes:\n",
    "                    self.__classes.append(intent[\"tag\"])\n",
    "\n",
    "        # for each word in the vocabulary, use the lemmatizer to remove any unneccesary suffixes\n",
    "        self.__vocab = [\n",
    "            self.__lemmatizer.lemmatize(word)\n",
    "            for word in self.__vocab\n",
    "            if word not in self.__IGNORE_CHAR\n",
    "        ]\n",
    "\n",
    "        # create a sorted set of the vocab, and a sorted list of the classes\n",
    "        self.__vocab = sorted(set(self.__vocab))\n",
    "        self.__classes = sorted(self.__classes)\n",
    "\n",
    "    def __create_training_data(self):\n",
    "        \"\"\"\n",
    "        Vectorises the data and splits the data into 'bags' and 'rows'.\n",
    "        \"\"\"\n",
    "        # iterates through each item in the self.__documents\n",
    "        for document in self.__documents:\n",
    "            # Matrix where each row represents the vocabulary.\n",
    "            # If the word in the vocabulary is also in the document, then that will be 1 to show that it is present in both.\n",
    "            bag = []\n",
    "\n",
    "            # gets the tokenised words\n",
    "            word_patterns = document[0]\n",
    "\n",
    "            # converts all words to lowercase and the lemmatizer reduces it to its base dictionary form\n",
    "            word_patterns = [\n",
    "                self.__lemmatizer.lemmatize(word.lower()) for word in word_patterns\n",
    "            ]\n",
    "\n",
    "            # iterates through each word in vocab\n",
    "            # create a vectorised from of the user input\n",
    "            # against the vocabulary, if there is an instance in the user input, make that associated index 1.\n",
    "            for word in self.__vocab:\n",
    "                bag.append(1) if word in word_patterns else bag.append(0)\n",
    "\n",
    "            # creates an output row that is as long as the length of the classes\n",
    "            # each index corresponds to a class\n",
    "            # sets the index corresponding to the class tag to 1\n",
    "            output_row = list([0] * len(self.__classes))\n",
    "            output_row[self.__classes.index(document[1])] = 1\n",
    "\n",
    "            # append the bag of word and the output row to the training list as one list\n",
    "            self.__training.append(bag + output_row)\n",
    "\n",
    "        # shuffles the data, forces the model to learn patterns from relationships rather than sequence of data\n",
    "        random.shuffle(self.__training)\n",
    "\n",
    "        # converts into a numpy array, more efficent and can be easily manipulated, which is needed from training\n",
    "        self.__training = np.array(self.__training)\n",
    "\n",
    "        # splits the data into the bag (train_x) and row (train_y)\n",
    "        self.__train_x = self.__training[:, : len(self.__vocab)]\n",
    "        self.__train_y = self.__training[:, len(self.__vocab) :]\n",
    "\n",
    "    def __create_model(self):\n",
    "        \"\"\"\n",
    "        Prepares the model for training by creating, optimising and then compiling the model.\n",
    "        \"\"\"\n",
    "\n",
    "        # Prepares the model for training #\n",
    "\n",
    "        self.__model = tf.keras.Sequential(\n",
    "            [\n",
    "                # creates the first later with 128 neurons\n",
    "                # input_shape tells the model of how many inputs to expect.\n",
    "                # uses a Reftified Linear Unit, which allows the model to learn from more complex patterns\n",
    "                tf.keras.layers.Dense(\n",
    "                    128, input_shape=(len(self.__train_x[0]),), activation=\"relu\"\n",
    "                ),\n",
    "                # Sets half the inputs to 0, so that it learns patterns rather than details\n",
    "                # makes predictions less reliant on a single neuron/node\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                # Creates the second later with 64 neurons\n",
    "                tf.keras.layers.Dense(64, activation=\"relu\"),\n",
    "                # preventing overfitting again\n",
    "                tf.keras.layers.Dropout(0.5),\n",
    "                # output layer, with the same number of neutrons as the output vector\n",
    "                # softmax used to give a probability over the differnt classes, that sum to 1.\n",
    "                tf.keras.layers.Dense(len(self.__train_y[0]), activation=\"softmax\"),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # optimises the model\n",
    "        # learning  => controls the size of the steps the optimizer uses when balancing the weights\n",
    "        # momentum  => dictates how much of the previous data is used to make the current update\n",
    "        # nesterov  => updates the weights and makes corrections quickly, results in faster processing.\n",
    "\n",
    "        sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
    "\n",
    "        # Prepares the model for training.\n",
    "        # loss      => how good the model is compared to the actual data, measures the difference between the predicted probabilities and the actual distribution\n",
    "        # optimizer => changes the attribues liek weight and learning rate reduce loss\n",
    "        # metrics   => used to evaluate performace, how correct its predictions are\n",
    "\n",
    "        self.__model.compile(\n",
    "            loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"]\n",
    "        )\n",
    "\n",
    "    def __train_model(self):\n",
    "        \"\"\"\n",
    "        Trains the model on the given data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Responsible for training the model using the data.\n",
    "        # epoch      => how many times the model will iterate over the training databset\n",
    "        # batch_size => model will update its weights after 5 samples\n",
    "        # verbose    => showing the information from training (0 -> show nothing, 1-> show progress bar, 2 -> more in depth)\n",
    "        self.hist = self.__model.fit(\n",
    "            self.__train_x, self.__train_y, epochs=200, batch_size=5, verbose=1\n",
    "        )\n",
    "\n",
    "    def __save_model(self, model_file=\"model.h5\"):\n",
    "        \"\"\"\n",
    "        Saves the model, words and classes to a file, which is needed for generating responses.\n",
    "\n",
    "        Args:\n",
    "            model_file (str, optional): The name of the file that the model will be saved to. Defaults to 'model.h5'.\n",
    "        \"\"\"\n",
    "\n",
    "        # Saves the models, words and classes\n",
    "        self.__model.save(model_file, self.hist)\n",
    "        with open(\"vocab.pkl\", \"wb\") as file:\n",
    "            pickle.dump(self.__vocab, file)\n",
    "        with open(\"classes.pkl\", \"wb\") as file:\n",
    "            pickle.dump(self.__classes, file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chatbot = Chatbot()\n",
    "    chatbot.create_model(\"intents.json\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
